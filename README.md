# IR-Project
In the project titled "Exploring Language Model Prompt Personalization and Context Retaining Summarization," conducted at the University of Massachusetts Amherst, my focus was on advancing the field of personalized language models (PLMs). The project delved into two crucial aspects: semantic relationship extraction and context-retaining summarization. Leveraging the LaMP benchmark datasets, specifically "Personalized News Categorization" and "Personalized Product Rating," I employed a user-based separation setting to investigate the efficacy of personalized models over un-personalized counterparts. The research involved implementing two distinct approaches. The first approach utilized a zero-shot pre-trained language model to capture semantic relationships between user queries and historical interactions, enhancing the model's understanding. The second approach focused on context-retaining summarization of user profiles, contributing to more nuanced and personalized interactions. Through rigorous evaluation using metrics like accuracy, F1 score, MAE, and RMSE, the project aimed to uncover the strengths and limitations of personalized language models, providing valuable insights for future developments in natural language understanding and generation.
